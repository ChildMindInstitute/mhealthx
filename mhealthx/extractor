#!/usr/bin/env python
"""
This pipeline runs feature extraction on mHealth data stored on Synapse.org.

For help in using extractor ::

    - README file
    - Help on the command line::

        $ extractor --help

This file uses Nipype (http://www.nipy.org/nipype/) to create a workflow
environment that enables extractor to run in a flexible, modular manner
while storing provenance information.

Authors:
    - Arno Klein, 2015  (arno@sagebase.org)  http://binarybottle.com

Copyright 2015,  Sage Bionetworks (http://sagebase.org), Apache v2.0 License

"""

import os
import argparse
from nipype import config, logging
from nipype.interfaces.io import DataGrabber, DataSink
from nipype.interfaces.utility import Function as Fn
from nipype.interfaces.utility import IdentityInterface
from nipype.pipeline.engine import Workflow, Node
from mhealthx.io_data import read_synapse_table_files

#=============================================================================
#
#   Command-line arguments
#
#=============================================================================
parser = argparse.ArgumentParser(description="""
                    The extractor software extracts features from mHealth
                    data stored on Sage Bionetwork's Synapse.org.""",
                     formatter_class = lambda prog:
                     argparse.HelpFormatter(prog, max_help_position=40))
parser.add_argument("-v", "--version", help="version number",
                    action='version', version='%(prog)s 0.1')
parser.add_argument("-n", "--numproc",
                    help=('number of processors (default: 1)'),
                    type=int, default=1, metavar='INT')
parser.add_argument("-g", "--graph",
                    help=('plot workflow graph: "hier", "flat", "exec"'),
                    choices=['hier', 'flat', 'exec'], metavar='STR')
parser.add_argument("--plugin", dest="plugin",
                    default='Linear',
                    help="optional plugin: --plugin PBS")
parser.add_argument("--plugin_args", dest="plugin_args",
                    help="optional plugin arguments:"
                         " --plugin_args \"dict(qsub_args='-q many')\"")
login_group = parser.add_argument_group('Synapse login')
login_group.add_argument("-e", "--email",
                    help="email address to access Synapse project",
                    metavar='STR')
login_group.add_argument("-p", "--password",
                    help="password to access Synapse project",
                    metavar='STR')
activities_group = parser.add_argument_group('activities')
activities_group.add_argument("--phonation",
                    help="Synapse table ID for phonation data",
                    metavar='STR')
activities_group.add_argument("--balance",
                    help="Synapse table ID for balance data",
                    metavar='STR')
activities_group.add_argument("--tapping",
                    help="Synapse table ID for tapping data",
                    metavar='STR')
outputs_group = parser.add_argument_group('outputs')
outputs_group.add_argument("--output",
                    help='output folder (if not $HOME/mhealthx_output)',
                    default=os.path.join(os.environ['HOME'],
                                         'mhealthx_output'), metavar='STR')
outputs_group.add_argument("--working",
                    help="working folder"
                         " (if not $HOME/mhealthx_working)",
                    default=os.path.join(os.environ['HOME'],
                                         'mhealthx_working'), metavar='STR')
args = parser.parse_args()
synapse_email = args.email
synapse_password = args.password
synID_phonation = args.phonation
synID_balance = args.balance
synID_tapping = args.tapping

#=============================================================================
#
#   Initialize main workflow and create output directories
#
#=============================================================================
main_workflow_name = 'mHealthX'
Flow = Workflow(name=main_workflow_name)
Flow.base_dir = args.working
Sink = Node(DataSink(), name='Results')
Sink.inputs.base_directory = args.output
if not os.path.isdir(args.output):
    print("Create missing output directory: {0}".format(args.output))
    os.makedirs(args.output)
if not os.path.isdir(args.working):
    print("Create missing working directory: {0}".format(args.working))
    os.makedirs(args.working)
if synID_phonation:
    phonation_file_path = os.path.join(args.working, main_workflow_name,
                                       'phonation_files')
    if not os.path.isdir(phonation_file_path):
        os.makedirs(phonation_file_path)

#=============================================================================
#
#   Phonation workflow
#
#=============================================================================
if synID_phonation:
    phonFlow = Workflow(name='phonation')

    #-------------------------------------------------------------------------
    # Read Synapse table:
    #-------------------------------------------------------------------------
    GetPhonationFiles = Node(name='get_phonation_files',
                             interface=Fn(function=read_synapse_table_files,
                                          input_names=['synapse_table_ID',
                                                       'synapse_email',
                                                       'synapse_password',
                                                       'column_name',
                                                       'select_rows',
                                                       'output_path'],
                                          output_names=['dataframe',
                                                        'files']))
    Flow.add_nodes([GetPhonationFiles])
    GetPhonationFiles.inputs.synapse_table_ID = synID_phonation
    GetPhonationFiles.inputs.synapse_email = synapse_email
    GetPhonationFiles.inputs.synapse_password = synapse_password
    GetPhonationFiles.inputs.column_name = 'audio_audio.m4a'
    GetPhonationFiles.inputs.select_rows = range(3)
    GetPhonationFiles.inputs.output_path = phonation_file_path

    #-------------------------------------------------------------------------
    # Rename m4a files:
    #-------------------------------------------------------------------------
    # RenamePhonationFiles = Node(name='rename_m4a_files',
    #                             interface=Fn(function=append_file_names,
    #                                          input_names=['input_files',
    #                                                       'file_append'],
    #                                          output_names=['output_files']))
    # Flow.add_nodes([RenamePhonationFiles])
    # Flow.connect(GetPhonationTable, 'dataframe',
    #              RenamePhonationFiles, 'dataframe')
    # Flow.connect(GetPhonationTable, 'schema',
    #              RenamePhonationFiles, 'schema')
    # RenamePhonationFiles.inputs.synapse_email = synapse_email
    # RenamePhonationFiles.inputs.synapse_password = synapse_password
    # Flow.connect(RenamePhonationFiles, 'file', Sink, 'phonation.@m4a_files')
    #
    # #-------------------------------------------------------------------------
    # # Convert voice file from M4A (AAC) to WAV format:
    # #-------------------------------------------------------------------------
    # ConvertPhonationFile = Node(name='convert_phonation_file',
    #                         interface=Fn(function=m4a_to_wav,
    #                                      input_names=['m4a_file'],
    #                                      output_names=['wav_file']))
    # Flow.add_nodes([ConvertPhonationFile])
    # Flow.connect(GetPhonationFile, 'file',
    #              ConvertPhonationFile, 'm4a_file')
    # Flow.connect(ConvertPhonationFile, 'wav_file',
    #              Sink, 'phonation.@wav_file')

    # #-------------------------------------------------------------------------
    # # Run openSMILE feature extraction on voice data:
    # #-------------------------------------------------------------------------
    # RunOpenSMILE = Node(name='openSMILE',
    #                     interface=Fn(function=run_openSMILE,
    #                                  input_names=['wav_file'],
    #                                  output_names=['features']))
    # Flow.add_nodes([RunOpenSMILE])
    # Flow.connect(ConvertPhonationFile, 'wav_file',
    #              RunOpenSMILE, 'wav_file')
    # Flow.connect(RunOpenSMILE, 'features',
    #              Sink, 'phonation.@features.openSMILE')

#=============================================================================
#
#   Run workflows
#
#=============================================================================
if __name__ == '__main__':

    from time import time
    time0 = time()
    #-------------------------------------------------------------------------
    # Set the workflow configuration to enable provenance tracking:
    #-------------------------------------------------------------------------
    #config.enable_provenance()

    #-------------------------------------------------------------------------
    # Set the workflow configuration to use content hashing:
    #-------------------------------------------------------------------------
    Flow.config['execution']['hash_method'] = 'content'
    #hashing = args.hashing
    #if hashing:
    #Flow.config['execution']['hash_method'] = 'content'
    #Flow.config['execution']['use_relative_paths'] = True

    #-------------------------------------------------------------------------
    # Generate a visual graph:
    #-------------------------------------------------------------------------
    graph_vis = args.graph
    if graph_vis == 'hier':
        graph_vis = 'hierarchical'
    if graph_vis:
        if graph_vis == 'exec':
            Flow.write_graph(graph2use=graph_vis, simple_form=False)
        else:
            Flow.write_graph(graph2use=graph_vis)

    #-------------------------------------------------------------------------
    # Debug: http://nipy.org/nipype/users/config_file.html#debug-configuration
    #-------------------------------------------------------------------------
    debug = False
    if debug:
        config.set('logging', 'workflow_level', 'DEBUG')
        logging.update_logging(config)
        Flow.config['execution']['stop_on_first_rerun'] = True
        nproc = 1
    else:
        if args.numproc:
            nproc = args.numproc
        else:
            nproc = 1

    #-------------------------------------------------------------------------
    # Run plugin, such as for multiple processes:
    #-------------------------------------------------------------------------
    if args.plugin:
        if args.plugin_args:
            Flow.run(args.plugin, plugin_args=eval(args.plugin_args))
        else:
            Flow.run(args.plugin)
    if args.numproc:
        if nproc > 1:
            Flow.run(plugin='MultiProc',
                     plugin_args={'n_procs': args.numproc})
                     #updatehash=True)
        else:
            Flow.run()  #updatehash=True)
    else:
        Flow.run()  #updatehash=True)
    # # Default is to use all processors:
    #else:
    #    Flow.run(plugin='MultiProc')

    print('extractor run finished ({0:0.2f} seconds).'.format(time() - time0))
